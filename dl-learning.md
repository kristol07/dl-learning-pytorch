
目录：
- [线性回归](#%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92)
- [Softmax 回归](#softmax-%e5%9b%9e%e5%bd%92)
  - [交叉熵损失函数](#%e4%ba%a4%e5%8f%89%e7%86%b5%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0)
- [多层感知机](#%e5%a4%9a%e5%b1%82%e6%84%9f%e7%9f%a5%e6%9c%ba)
  - [常见的激活函数](#%e5%b8%b8%e8%a7%81%e7%9a%84%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0)
- [过拟合、欠拟合](#%e8%bf%87%e6%8b%9f%e5%90%88%e6%ac%a0%e6%8b%9f%e5%90%88)
  - [应对过拟合](#%e5%ba%94%e5%af%b9%e8%bf%87%e6%8b%9f%e5%90%88)
- [梯度消失、梯度爆炸](#%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1%e6%a2%af%e5%ba%a6%e7%88%86%e7%82%b8)
  - [随机初始化模型参数](#%e9%9a%8f%e6%9c%ba%e5%88%9d%e5%a7%8b%e5%8c%96%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0)
- [自然语言处理（待完成）](#%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e5%a4%84%e7%90%86%e5%be%85%e5%ae%8c%e6%88%90)
  - [文本预处理](#%e6%96%87%e6%9c%ac%e9%a2%84%e5%a4%84%e7%90%86)
    - [步骤](#%e6%ad%a5%e9%aa%a4)
  - [语言模型](#%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b)
    - [n元语法(n-gram)](#n%e5%85%83%e8%af%ad%e6%b3%95n-gram)
  - [循环神经网络基础](#%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%9f%ba%e7%a1%80)
  - [循环神经网络进阶](#%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%9b%e9%98%b6)
  - [word2vec](#word2vec)
  - [Seq2Seq 模型](#seq2seq-%e6%a8%a1%e5%9e%8b)
  - [注意力机制](#%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6)
  - [Transformer 模型](#transformer-%e6%a8%a1%e5%9e%8b)
- [卷积神经网络](#%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c)
  - [互相关运算与卷积运算](#%e4%ba%92%e7%9b%b8%e5%85%b3%e8%bf%90%e7%ae%97%e4%b8%8e%e5%8d%b7%e7%a7%af%e8%bf%90%e7%ae%97)
  - [特侦图与感受野](#%e7%89%b9%e4%be%a6%e5%9b%be%e4%b8%8e%e6%84%9f%e5%8f%97%e9%87%8e)
  - [填充和步幅](#%e5%a1%ab%e5%85%85%e5%92%8c%e6%ad%a5%e5%b9%85)
  - [池化](#%e6%b1%a0%e5%8c%96)
- [卷积神经网络进阶（模型）](#%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e8%bf%9b%e9%98%b6%e6%a8%a1%e5%9e%8b)
  - [LeNet 模型](#lenet-%e6%a8%a1%e5%9e%8b)
  - [AlexNet 模型](#alexnet-%e6%a8%a1%e5%9e%8b)
    - [图像增广](#%e5%9b%be%e5%83%8f%e5%a2%9e%e5%b9%bf)
      - [常用的图像增广方法](#%e5%b8%b8%e7%94%a8%e7%9a%84%e5%9b%be%e5%83%8f%e5%a2%9e%e5%b9%bf%e6%96%b9%e6%b3%95)
  - [VGG：使用重复元素的网络](#vgg%e4%bd%bf%e7%94%a8%e9%87%8d%e5%a4%8d%e5%85%83%e7%b4%a0%e7%9a%84%e7%bd%91%e7%bb%9c)
  - [NiN：⽹络中的⽹络](#nin%e2%bd%b9%e7%bb%9c%e4%b8%ad%e7%9a%84%e2%bd%b9%e7%bb%9c)
  - [GoogLeNet 模型](#googlenet-%e6%a8%a1%e5%9e%8b)

## 线性回归

- 模型: $y=WX+b$

- 损失函数：均方误差函数

- 优化函数：小批量随机梯度下降
  - 数值方法优化求解

## Softmax 回归

多分类问题
- 输入：$x_1$, $x_2$, $x_3$
- 输出：$o_1$, $o_2$
- 结果：$y=argmax(softmax($$o_i$$))$

直接使用输出层作为输出的问题：
- 输出值范围不定，难以直观判断这些值的意义，[1,10,1]vs[0.1,10,0.1]
- 真实标签的离散值与不确定范围的输出值之间的误差难以衡量

使用softmax解决以上问题，其将输出值转换为正且和为1的概率分布，同时不改变预测类别的输出。

### 交叉熵损失函数

更适合衡量两个概率分布差异的测量函数

## 多层感知机

深度神经网络基础

$H=\phi(XW_h+bh)$

$O=HW_o+b_o$


如果全连接层只是对数据进行仿射变换，那么最后本质上还是一侧单层神经网络。解决方法之一是引入非线性变换，这种函数叫做“激活函数”。

### 常见的激活函数
- ReLU： $max(x,0)$
- Sigmoid: $\frac{1}{1+exp(-x)}$
  - 将元素的值变换到0和1之间
- tanh: $\frac{1-exp(-2x)}{1+exp(-2x)}$
  - 当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。

关于激活函数的选择
- 由于梯度消失问题，有时要避免使用sigmoid和tanh函数
- 在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少

## 过拟合、欠拟合

训练误差 vs 泛化误差~测试误差

严格讲，测试集只能在模型确定后用一次。 => 预留验证集用于模型选择

K折交叉验证：
数据分成K份，做K次模型训练（K-1）和验证（1）

过拟合、欠拟合的重要因素：模型复杂度和训练数据集大小

### 应对过拟合

方法：
- 正则化：通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制
  - $L_2$范数正则化又叫权重衰减
- 丢弃法
  - 对该隐藏层使用丢弃法，该层的隐藏单元将有一定概率被丢弃掉，且不改变其输入的期望值

## 梯度消失、梯度爆炸

深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。当神经网络的层数较多时，模型的数值稳定性容易变差。

- 协变量偏移
  - 协变量变化是因为问题的根源在于特征分布的变化（即协变量的变化）。数学上，我们可以说$P(x)$改变了，但$P(y|x)$保持不变。
  - 训练数据使用的是猫和狗的真实的照片，但是在测试时，我们被要求对猫和狗的卡通图片进行分类。
- 标签偏移
- 概念偏移

### 随机初始化模型参数

Xavier随机初始化：
它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。


## 自然语言处理（待完成）

### 文本预处理

#### 步骤
1. 读入文本
2. 分词：句子->词（token）
3. 建立字典，将每个词映射到一个唯一的索引（index）
4. 将文本从词的序列转换为索引的序列，方便输入模型

更好的分词工具：[spaCy](https://spacy.io/) 和 [NLTK](https://www.nltk.org/)

### 语言模型

语言模型的目标：
评估一个词序列出现是否合理，即计算该序列的概率：$P(w_1,...,w_T)$

#### n元语法(n-gram)
- 基于统计的语言模型
- n元语法通过马尔可夫假设简化模型，马尔科夫假设是指一个词的出现只与前面n个词相关
- 可能的缺陷：
  - 参数空间过大
  - 数据稀疏

### 循环神经网络基础

并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。

与多层感知机不同的是，这里我们保存上一时间步的隐藏变量$H_{t−1}$​，并引入一个新的权重参数$W_{hh}∈R^{h×h}$，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量: $H_t=\phi(X_tW_{xh}+H_{t-1}W_{hh}+b_h)$

=> 基于字符级循环神经网络的语言模型

### 循环神经网络进阶

### word2vec

### Seq2Seq 模型

### 注意力机制

### Transformer 模型

## 卷积神经网络

### 互相关运算与卷积运算

卷积核的尺寸通常小于输入数组，卷积核在输入数组上滑动，在每个位置上，卷积核与该位置处的输入子数组按元素相乘并求和，得到输出数组中相应位置的元素。

卷积层得名于卷积运算，但卷积层中用到的并非卷积运算而是互相关运算。我们将核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，所以使用互相关运算与使用卷积运算并无本质区别。

### 特侦图与感受野

二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素$x$的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做$x$的感受野（receptive field）。

我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。

### 填充和步幅

卷积层的两个超参数，即填充和步幅，它们可以对给定形状的输入和卷积核改变输出形状。

### 池化

池化层主要用于缓解卷积层对位置的过度敏感性。

同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做最大池化或平均池化。

## 卷积神经网络进阶（模型）

### LeNet 模型

LeNet分为卷积层块和全连接层块两个部分。

卷积层块里的基本单位是卷积层后接平均池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的平均池化层则用来降低卷积层对位置的敏感性。
卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用5x5的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。

全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。

### AlexNet 模型

特征：
- 8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。
- 将sigmoid激活函数改成了更加简单的ReLU激活函数。
- 用Dropout来控制全连接层的模型复杂度。
- 引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。

![LeNet and AlexNet](https://cdn.kesci.com/upload/image/q5kv4gpx88.png?imageView2/0/w/640/h/640)

#### 图像增广

图像增广（image augmentation）技术通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。图像增广的另一种解释是，随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。例如，我们可以对图像进行不同方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性。我们也可以调整亮度、色彩等因素来降低模型对色彩的敏感度。

##### 常用的图像增广方法

- 翻转和裁剪
  - 在“池化层”里我们解释了池化层能降低卷积层对目标位置的敏感度。除此之外，我们还可以通过对图像随机裁剪来让物体以不同的比例出现在图像的不同位置，这同样能够降低模型对目标位置的敏感性。
- 变化颜色
  - 我们可以从4个方面改变图像的颜色：亮度（brightness）、对比度（contrast）、饱和度（saturation）和色调（hue）。
- 叠加多个图像增广方法



### VGG：使用重复元素的网络

通过重复使⽤简单的基础块来构建深度模型。

Block: 数个相同的填充为1、窗口形状为3x3的卷积层,接上一个步幅为2、窗口形状为2x2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。

### NiN：⽹络中的⽹络

- LeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取 空间特征，再以由全连接层构成的模块来输出分类结果。
- NiN：串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。

1×1卷积核作用
1. 放缩通道数：通过控制卷积核的数量达到通道数的放缩。
2. 增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。
3. 计算参数少

NiN重复使⽤由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层⽹络。
NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数 的NiN块和全局平均池化层。
NiN的以上设计思想影响了后⾯⼀系列卷积神经⽹络的设计。

### GoogLeNet 模型

由Inception基础块组成。
- Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。
- 可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。

![GoogLeNet 完整模型结构](https://cdn.kesci.com/upload/image/q5l6x0fyyn.png?imageView2/0/w/640/h/640)

